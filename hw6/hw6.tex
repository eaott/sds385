\documentclass{article}
\usepackage[top=.5in, bottom=.5in, left=.9in, right=.9in]{geometry}
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{mathtools}

\newcommand{\obar}[1]{\ensuremath{\overline{ #1 }}}
\newcommand{\iid}{\ensuremath{\stackrel{\textrm{iid}}{\sim}}}

\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0,0.25,0}
\newcommand{\soln}{{\color{red}\textbf{Solution:~}\color{black}}}


\usepackage[formats]{listings}
\lstdefineformat{R}{~={\( \sim \)}}
\lstset{% general command to set parameter(s)
basicstyle=\small\ttfamily, % print whole listing small
keywordstyle=\bfseries\rmfamily,
keepspaces=true,
% underlined bold black keywords
commentstyle=\color{darkgreen}, % white comments
stringstyle=\ttfamily, % typewriter type for strings
showstringspaces=false,
numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt, %
frame=shadowbox,
rulesepcolor=\color{black},
,columns=fullflexible,format=R
} %
\renewcommand{\ttdefault}{cmtt}
% enumerate is numbered \begin{enumerate}[(I)] is cap roman in parens
% itemize is bulleted \begin{itemize}
% subfigures:
% \begin{subfigure}[b]{0.5\textwidth} \includegraphics{asdf.jpg} \caption{} \label{subfig:asdf} \end{subfigure}
\hypersetup{colorlinks=true, urlcolor=blue, linkcolor=blue, citecolor=red}


\graphicspath{ {C:/Users/Evan/Desktop/} }
\title{\vspace{-6ex}HW 6\vspace{-2ex}}
\author{Evan Ott \\ UT EID: eao466\vspace{-2ex}}
%\date{DATE}
\setcounter{secnumdepth}{0}
\usepackage[parfill]{parskip}



\begin{document}
\maketitle
\section{Proximal operators}
\subsection{(A)}
\begin{align*}
f(x)&\approx \hat{f}(x;x_0)=f(x_0)+(x-x_0)^\top\nabla f(x_0)\\
\textrm{prox}_\gamma \hat{f}(x)&=\arg\min_z \left[ \hat{f}(z) + \frac{1}{2\gamma} \lVert z-x\rVert_2^2 \right]\\
&=\arg\min_z \left[ f(x_0)+(z-x_0)^\top\nabla f(x_0) + \frac{1}{2\gamma} \lVert z-x\rVert_2^2 \right]\\
0&=\frac{\partial}{\partial z}\left[ f(x_0)+(z-x_0)^\top\nabla f(x_0) + \frac{1}{2\gamma} \lVert z-x\rVert_2^2 \right]\\
&=0 + \nabla f(x_0) + \frac{1}{2\gamma}(2z^* - 2x)=\nabla f(x_0) + \frac{1}{\gamma}(z^* - x)\\
\textrm{prox}_\gamma \hat{f}(x)=z^*&=x-\gamma \nabla f(x_0)
\end{align*}
which is indeed the gradient-descent step for $f(x)$ of size $\gamma$ starting at $x_0$.

\subsection{(B)}
\begin{align*}
l(x)&=\frac{1}{2}x^\top P x - q^\top x + r\\
\textrm{prox}_{1/\gamma}l(x)&=\arg\min_z \left[ \hat{l}(z) + \frac{\gamma}{2} \lVert z-x\rVert_2^2 \right]\\
&= \arg\min_z \left[ \frac{1}{2}z^\top P z - q^\top z + r + \frac{\gamma}{2} \lVert z-x\rVert_2^2 \right]\\
0&=\frac{\partial}{\partial z}\left[ \frac{1}{2}z^\top P z - q^\top z + r + \frac{\gamma}{2} \lVert z-x\rVert_2^2 \right]\\
&=Pz^* - q + \gamma (z^* - x)\\
\gamma x + q&= \left(P + \gamma I\right) z^*\\
\textrm{prox}_{1/\gamma}l(x)=z^*&=\left(P + \gamma I\right)^{-1}(\gamma x + q)
\end{align*}
assuming $\left(P + \gamma I\right)^{-1}$ exists.

If we have $ y | x \sim N(Ax, \Omega^{-1})$ with $y$ having $n$ rows, then
\begin{align*}
L(y | x)&=\frac{1}{\sqrt{2\pi}^n} |\Omega|^{-1/2} \exp\left[- \frac{1}{2}(y-Ax)^\top \Omega^{-1} (y-Ax)\right]\\
n(y|x)=-\log L(y|x)&=\frac{1}{2}\log |\Omega| +\frac{n}{2}\log(2\pi) + \frac{1}{2}(y-Ax)^\top \Omega^{-1} (y-Ax)\\
&=\frac{1}{2}y^\top \Omega^{-1} y - (Ax)^\top\Omega^{-1} y + \frac{1}{2}(Ax)^\top\Omega^{-1} Ax + \frac{1}{2}\log |\Omega| +\frac{n}{2}\log(2\pi)
\end{align*}
So $P=\Omega^{-1}$, $q=\Omega^{-1}Ax$ (because $\Omega=\Omega^\top$ since it is a covariance matrix), and $r=\frac{1}{2}(Ax)^\top\Omega^{-1} Ax + \frac{1}{2}\log |\Omega| +\frac{n}{2}\log(2\pi)$.


\subsection{(C)}
\begin{align*}
\phi(x)&=\tau \lVert x\rVert_1\\
\textrm{prox}_\gamma \phi(x)&=\arg\min_z \left[ \phi(z) + \frac{1}{2\gamma} \lVert z-x\rVert_2^2 \right]\\
&=\arg\min_z \left[ \tau \lVert z \rVert_1 + \frac{1}{2\gamma} \lVert z-x\rVert_2^2 \right]\\
&=\arg\min_z \left[ \tau \sum_{i=1}^n \left(|z_i|\right) + \frac{1}{2\gamma} \sum_{i=1}^n\left( (z_i-x_i)^2 \right) \right]\\
&=\arg\min_z \left[\sum_{i=1}^n \frac{1}{2\gamma}(z_i-x_i)^2 + \tau |z_i| \right]\\
&=\arg\min_z \left[\sum_{i=1}^n \frac{1}{2}(z_i-x_i)^2 + \tau\gamma |z_i| \right]~~~~{\small\textrm{(multiplying by positive scalar yields same optimization)}}
\end{align*}
The term being minimized for each component $z_i$ is exactly $S_{\tau\gamma}(x_i)$ from the notation last week, and
there are no interaction terms between the $z_i$ and $z_j$ for $i\neq j$, so
\begin{align*}
\left(\textrm{prox}_\gamma \phi(x)\right)_i=S_{\tau\gamma}(x_i)
\end{align*}


\section{The proximal gradient method}
\subsection{(A)}
\begin{align*}
\hat{x}&=\arg\min_x \left\{ \tilde{l}(x;x_0)+\phi(x) \right\}\\
&=\arg\min_x \left\{ l(x_0) + (x-x_0)^\top \nabla l(x_0) + \frac{1}{2\gamma} \lVert x-x_0 \rVert_2^2+\phi(x) \right\}\\
&=\arg\min_z \left\{ l(x_0) + (z-x_0)^\top \nabla l(x_0) + \frac{1}{2\gamma} \lVert z-x_0 \rVert_2^2+\phi(z) \right\}\\
&=\arg\min_z \left\{ \phi(z) + l(x_0) + (z-x_0)^\top \nabla l(x_0) + \frac{1}{2\gamma} \left(z^\top z - 2x_0^\top z + x_0^\top x_0 \right) \right\}\\
&=\arg\min_z \left\{ \phi(z) + (z-x_0)^\top \nabla l(x_0) + \frac{1}{2\gamma} \left(z^\top z - 2x_0^\top z + x_0^\top x_0 \right) \right\}~~~~{\small\textrm{(add/subtract a constant for same optimization)}}\\
&=\arg\min_z \left\{ \phi(z) + \frac{\gamma}{2}\left[\nabla l(x_0)\right]^\top\nabla l(x_0)+2\frac{1}{2\gamma}(z-x_0)^\top \gamma\nabla l(x_0) + \frac{1}{2\gamma} \left(z^\top z - 2x_0^\top z + x_0^\top x_0 \right) \right\}\\
&=\arg\min_z \left\{ \phi(z) + \frac{1}{2\gamma}\left[\gamma\nabla l(x_0)\right]^\top\gamma\nabla l(x_0)+2\frac{1}{2\gamma}(z-x_0)^\top \gamma\nabla l(x_0) + \frac{1}{2\gamma} \left(z^\top z - 2x_0^\top z + x_0^\top x_0 \right) \right\}\\
&=\arg\min_z \left\{ \phi(z) + \frac{1}{2\gamma}\left(\left[\gamma\nabla l(x_0)\right]^\top\gamma\nabla l(x_0)+2(z-x_0)^\top \gamma\nabla l(x_0) +  z^\top z - 2x_0^\top z + x_0^\top x_0 \right) \right\}\\
&=\arg\min_z \left[\phi(z) + \frac{1}{2\gamma} \lVert z - x_0 + \gamma\nabla l(x_0) \rVert_2^2\right]\\
&=\arg\min_z \left[\phi(z) + \frac{1}{2\gamma} \lVert z - (x_0 - \gamma\nabla l(x_0)) \rVert_2^2\right]\\
u&=x_0-\gamma\nabla l(x_0)\\
\hat{x}&=\textrm{prox}_\gamma \phi(u)\\
\end{align*}



\subsection{(B)}
Now, we want to play around with our results to cast the lasso regression into a proximal gradient problem.
\begin{align*}
\hat{\beta}&=\arg\min_\beta \left\{\lVert y-X\beta\rVert_2^2 + \lambda \lVert \beta\rVert_1\right\}\\
l(\beta | X,y)&=\lVert y-X\beta\rVert_2^2=y^\top y - 2y^\top X\beta + \beta^\top X^\top X \beta\\
\hat{\beta}&=\arg\min_\beta \left\{l(\beta | X,y)+ \lambda \lVert \beta\rVert_1\right\}\\
l(\beta | X,y)\approx\hat{l}(\beta | X,y ; \beta_0)&= l(\beta_0 | X, y) + (\beta-\beta_0)^\top \nabla l(\beta_0 | X, y)\\
\nabla l(\beta | X, y)&=0 -2X^\top y + 2X^\top X\beta\\
\hat{l}(\beta | X,y ; \beta_0)&=\lVert y-X\beta_0\rVert_2^2 + (\beta-\beta_0)^\top \left(-2X^\top y + 2X^\top X\beta_0\right)
\end{align*}
Now, in the linear approximation to $l(\beta | X,y)$, we add in the regularization:
\begin{align*}
\tilde{l}(\beta | X,y ; \beta_0)&=\lVert y-X\beta_0\rVert_2^2 + (\beta-\beta_0)^\top \left(-2X^\top y + 2X^\top X\beta_0\right)+\frac{1}{2\gamma}\lVert \beta-\beta_0\rVert_2^2
\end{align*}
Now, we let $l(\beta|X, y)\approx \tilde l(\beta | X,y ;\beta_0)$ when $\beta$ is near $\beta_0$.
This is now exactly the form of surrogate optimization referenced above so
\begin{align*}
\phi(\beta)&=\lambda \lVert \beta \rVert_1\\
u^{(t)}&=\beta^{(t)}-\gamma^{(t)} \nabla l(\beta^{(t)} | X, y)= \beta^{(t)} - \gamma^{(t)}\left(2X^\top X\beta^{(t)}-2X^\top y\right)\\
\beta^{(t+1)}&=\textrm{prox}_{\gamma^{(t)}} \phi(u^{(t)})\\
\beta_i^{(t+1)}&=S_{\lambda\gamma^{(t)}}\left(u_i^{(t)}\right)=\textrm{sign}\left(u_i^{(t)}\right)\left(\left|u_i^{(t)}\right|-\lambda\gamma^{(t)}\right)_+
\end{align*}
So, to go from step $t$ to step $t+1$, we just compute $u^{(t)}$ then use its components to compute $\beta_i^{(t+1)}$.

There's a relatively high one-time cost to compute $X^\top X$ and $X^\top y$, and (depending on how big $p$, the number of elements of $\beta$, is) this cost carries over each iteration to compute $X^\top X \beta^{(t)}$. That's a $O(p^2)$ calculation (at least in the dense case). Beyond that, the rest of the operations are $O(p)$.




\section{Notes from class Oct. 17}
Looking today at dual descent, which is the minimal pre-requisite to understand ADMM (hw 7). 

\subsection{Standard-form convex optimization problem}
Note: $x$ will be what we're optimizing.

Minimize $f_0(x)$ subject to $f_i(x) \leq 0$ for $i=1,\ldots, m$ and $Ax=b$ (affine), with $f_0$ and all $f_i$ being convex.

Convex set is geometric: take two points in the set, any point on the line between
them is also in the set. Convex function is similar: look at the affine transformation (I think linear approximation at a point) is a global under-estimator or not.

\subsubsection{Linear program (LP)}
Minimize $c^\top x + d$ subject to $Gx\preceq h$ and $Ax=b$ ($\preceq$ means pointwise inequality [applies the $\leq$ operator element-wise].

\subsubsection{Quadratic program (QP)}
Minimize $\frac{1}{2}x^\top P x + q^\top x + r$ subject to $Gx\preceq h$ and $Ax=b$.

For example, constrained least squares:

minimize $\frac{1}{2}\lVert Ax-b\rVert_2^2$ (which is the optimization way of writing $X\beta-y$), constrained by $l\preceq x \preceq u$. That is, $x\preceq u$ and $-x\preceq l$ which is an example of a QP ($G$ would be a block matrix with identity and negative identity).

\subsection{Slack variables}
Similar in idea to latent variables used in MCMC that augment the model, put it
in a bigger space, and rewrite the problem.

Example: $$\underset{x\in \mathbb{R}^D}{\operatorname{minimize}}~\frac{1}{2}\lVert Ax-b\rVert_2^2 + \lambda \lVert x\rVert_1$$
We rewrite it in as
\begin{align*}
\underset{x\in \mathbb{R}^D,~z\in\mathbb{R}^D}{\operatorname{minimize}}~&\frac{1}{2}\lVert Ax-b\rVert_2^2 + \lambda \lVert z\rVert_1\\
\textrm{subject to}&~x=z
\end{align*}

Example: $$\underset{x\in \mathbb{R}^D}{\operatorname{minimize}}~\frac{1}{2}\lVert x-y\rVert_2^2 + \lambda \lVert Dx\rVert_1$$ where $D$ is an ``oriented edge matrix'' (see spatial smoothing). Can rewrite as
\begin{align*}
\underset{x\in \mathbb{R}^D,~z\in\mathbb{R}^D}{\operatorname{minimize}}~&\frac{1}{2}\lVert x-y\rVert_2^2 + \lambda \lVert z\rVert_1\\
\textrm{subject to}&~Dx=z~~{\small \textrm{feasibility constraint}}
\end{align*}
Often, most algorithms don't enforce the feasibility constraint until convergence.

\subsection{Lagrangian}
Minimize $f_0(x)$ subject to $f_i(x)=\leq 0$ and $h_i(x)=0$ (not necessarily convex). Would like to cast this into an unconstrained optimization.

Define \begin{align*}
I\_(u)&=\left\{\begin{array}{cc}0 & u\leq 0\\\infty & \textrm{o.w.}\end{array}\right.\\
I_0(u)&=\left\{\begin{array}{cc}0 & u= 0\\\infty & \textrm{o.w.}\end{array}\right.
\end{align*}

So now
\begin{align*}
\underset{x\in \mathbb{R}^D}{\operatorname{minimize}}
f_0(x)+\sum_{i=1}^m I\_(f_i(x)) + \sum_{i=1}^p I_0(h_i(x))
\end{align*}
in other words, any time we're in a case where the constraint is not satisfied, our objective jumps to $\infty$.

The Lagrangian just linearizes $I\_(u)$ and $I_0(u)$:

\begin{align*}
L(x, \lambda, nu)&=f_0(x) +\sum_{i=1}^m \lambda_if_i(x) + \sum_{i=1}^p \nu_ih_i(x)
\end{align*}
where $\lambda_i$ and $\nu_i$ are the Lagrange multipliers (also called dual variables). $L(x, \lambda, \nu)$ is the ``primal variable,'' the thing we actually care about.

Now, let's look at the (Lagrange) dual function:
\begin{align*}
g(\lambda, \nu)=\inf_x L(x, \lambda, \nu)
\end{align*}
Why is this useful?

\subsubsection{Fact}
For any $\lambda \succeq 0, \nu$, we have
$g(\lambda, \nu)\leq p^*$ which is the optimal value of the primal problem ($f_0(x^*)$). 

\subsubsection{Proof}

For any $\lambda \succeq 0, \nu$, we have the following.
$$\sum_{i=1}^m \lambda_i f_i(\tilde{x}) + \sum_{i=1}^p\nu_i h_i(\tilde{x}) \leq 0$$
for any feasibile $\tilde{x}$ (all the $h_i$ are 0, all the $f_i \leq 0$ so this has to be true.

Therefore $L(\tilde{x}, \lambda, \nu)=f_0(\tilde{x}) + \sum_{i=1}^m \lambda_i f_i(\tilde{x}) + \sum_{i=1}^p\nu_i h_i(\tilde{x})\leq f_0(\tilde{x})$. And
$$g(\lambda, \nu)=\inf_x L(x, \lambda, \nu)\leq L(\tilde{x}, \lambda, \nu)\leq f_0(\tilde{x})$$
this works for any feasible $\tilde{x}$ so it must be true for the optimal value $x^*$.

\subsubsection{Dual problem}
$$\underset{\lambda\succeq 0,~ \nu\in\mathbb{R}^p}{\operatorname{maximize}}~ g(\lambda, \nu)$$
This is essentially a minimax problem: we're maximizing the lower bound. Let's say that the optimal value is $d^*=g(\lambda^*,~\nu^*)$.

Cool thing: strong (Lagrangian) duality is that $p^*=d^*$ which is kind of incredible, and this
is actually true sometimes. When? That's the \$1,000,000 question for math careers. However, in stats, it's true in basically all interesting convex problems (mostly). More particularly, under Slater's conditions (see Boyd \S5.2).

Now: slight change of notation to match the paper rather than matching the textbook. Dual variables $\lambda, \nu$ will now be denoted $y^*$.

If strong duality holds, then
$$x^*=\arg\min_x L(x, y^*)$$ where $y^*$ is a dual optimal solution (assuming that $L$ has one minimum). Why do we care? Sometimes it's easier to solve the dual problem than solving the primal problem.

\subsection{Dual ascent}
Now, let's assume that these conditions all hold (strong duality, one minimum, Slater's conditions).

{\textbf{Dual ascent}}: solve the dual problem by gradient ascent.

$y$ is the dual variable.
\subsubsection{Example}
minimize $f(x)$ subject to $Ax=b$. What's the lagrangian? Well, $Ax-b=0$ so
$$L(x, y)=f(x)+y^\top (Ax-b)$$

Dual ascent here is $y^{t+1}=y^t + \alpha^t \nabla g(y)$ where $g$ is the dual function $g(y)=\inf_x L(x, y)$.

How do we evaluate the gradient of the dual function? Think it's called the envelope formula (this is a general property of functions, with some regularities). For $g(y)=\inf_x L(x, y)$, we have:
$$\nabla g(y)=\left.\nabla_yL(x,y)\right|_{x=\hat{x}(y)}$$ where $\hat{x}(y)=\arg\min_x L(x, y)$

So in this case,
\begin{align*}
\nabla g(y)&=\nabla_y \left[f(x) + y^\top(Ax-b)\right]\\
&=Ax-b
\end{align*}
which is exactly the residuals of the feasibility constraints. So when $\nabla g(y)=0$ this gives the extremely interpretable result of having a solution when all constraints are met.

So dual ascent becomes:
\begin{align*}
x^{(t+1)}&=\arg\min_x L(x, y^{(t)})\\
y^{(t+1)}&=y^{(t)} + \alpha^{(t)} \left(Ax^{(t+1)}-b\right)
\end{align*}
which is nice because we never actually have to use the dual function. At convergence, $x^{(T)}=x^*$ and $y^{(T)}=y^*$.

This is most of the understanding we need for ADMM, but leaves out the method of multipliers (related to augmented Lagrangian).

\end{document}