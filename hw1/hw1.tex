\documentclass{article}
\usepackage[top=.5in, bottom=.5in, left=.9in, right=.9in]{geometry}
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{mathtools}

\newcommand{\obar}[1]{\ensuremath{\overline{ #1 }}}
\newcommand{\iid}{\ensuremath{\stackrel{\textrm{iid}}{\sim}}}

\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0,0.25,0}
\newcommand{\soln}{{\color{red}\textbf{Solution:~}\color{black}}}


\usepackage[formats]{listings}
\lstdefineformat{R}{~={\( \sim \)}}
\lstset{% general command to set parameter(s)
basicstyle=\small\ttfamily, % print whole listing small
keywordstyle=\bfseries\rmfamily,
keepspaces=true,
% underlined bold black keywords
commentstyle=\color{darkgreen}, % white comments
stringstyle=\ttfamily, % typewriter type for strings
showstringspaces=false,
numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt, %
frame=shadowbox,
rulesepcolor=\color{black},
,columns=fullflexible,format=R
} %
\renewcommand{\ttdefault}{cmtt}
% enumerate is numbered \begin{enumerate}[(I)] is cap roman in parens
% itemize is bulleted \begin{itemize}
% subfigures:
% \begin{subfigure}[b]{0.5\textwidth} \includegraphics{asdf.jpg} \caption{} \label{subfig:asdf} \end{subfigure}
\hypersetup{colorlinks=true, urlcolor=blue, linkcolor=blue, citecolor=red}


\graphicspath{ {C:/Users/Evan/Desktop/} }
\title{\vspace{-6ex}SDS385 HW 1\vspace{-2ex}}
\author{Evan Ott \\ UT EID: eao466\vspace{-2ex}}
%\date{DATE}
\setcounter{secnumdepth}{0}
\usepackage[parfill]{parskip}



\begin{document}
\maketitle
\section{Linear Regression}
\subsection{(A)}
WLS objective function:
\begin{align*}
\sum_{i=1}^N\frac{w_i}{2}(y_i-x_i^\top\beta)^2&=\frac{1}{2}\sum_{i=1}^Ny_iw_iy_i - \sum_{i=1}^Ny_iw_ix_i^\top\beta+\frac{1}{2}\sum_{i=1}^Nx_i^\top\beta w_ix_i^\top\beta\\
&=\frac{1}{2}y^\top W y - y^\top W X \beta + \frac{1}{2}(X\beta)^\top WX\beta\\
&=\frac{1}{2}(y-X\beta)^\top W(y-X\beta).
\end{align*}
Minimizing this function means setting the gradient (with respect to $\beta$) to zero:
\begin{align*}
\nabla_\beta\left[\frac{1}{2}(y-X\beta)^\top W(y-X\beta)\right]&=0
\end{align*}
That is
\begin{align*}
\nabla_\beta\left[\frac{1}{2}(y-X\beta)^\top W(y-X\beta)\right]&=0-(y^\top WX)^\top+\frac{2}{2}X^\top WX\hat{\beta}=0\\
&\Rightarrow (X^\top WX)\hat{\beta}=X^\top Wy~~~~_\blacksquare
\end{align*}

\subsection{(B)}
The matrix factorization idea basically amounts to trying to prevent the full inverse operation. Overall, you will still probably need something $O(n^3)$, but the constants matter when actually doing computation as opposed to asymptotics. We don't actually want the inverse anyway, we just want to solve $(X^\top WX)\hat{\beta}=X^\top Wy$ for $\beta$. Using a matrix
decomposition can help with that a lot.

I based my solution on the Cholesky decomposition (see \url{http://www.seas.ucla.edu/~vandenbe/103/lectures/chol.pdf}). This creates matrices $X=LL^\dagger$, where $L$ is lower-triangular. So, then solve $Lz=X^\top Wy$ for $z$ and $R\beta=z$ for $\beta$. The decomposition is still $O(n^3)$,
but faster than inverse. The two final steps are each $O(n^2)$ which are faster.

\subsection{(C)}
See code on GitHub (\texttt{hw1.R}).

\includegraphics[scale=0.5]{log_methods.png}

\subsection{(D)}
See code on GitHub (\texttt{hw1.R}).

\includegraphics[scale=0.5]{sparse.png}

\subsection{Notes from class}
Three main matrix decomposition techniques:
\begin{enumerate}[1.]
\item Cholesky $\rightarrow$ fast, unstable (susceptible to roundoff error)
\item QR $\rightarrow$ middle ground
\item SVD $\rightarrow$ slow, but works for close-to-rank-deficient matrices
\end{enumerate}

Using QR, we get $W^{1/2}X=QR$, where $R$ is $P\times P$ and upper-triangular (and thus invertible) and $Q$ is $N\times P$ with
orthonormal columns.
\begin{align*}
X^\top Wy&=X^\top W X\beta\\
X^\top W^{1/2} W^{1/2}y&=X^\top W^{1/2}W^{1/2}X\beta\\
(QR)^\top W^{1/2} y&=(QR)^\top QR\beta\\
Q^\top W^{1/2}y&=I R\beta=R\beta\\
\end{align*}

A note on R, \texttt{crossprod} computes $X^\top X$ but recognizes the symmetry so it takes half the time.


\section{Generalized linear models}
\subsection{(A)}
\begin{align*}
w_i(\beta)&=\frac{1}{1+\exp\{-x_i^\top\beta\}}\\
l(\beta)&=-\log\left\{\prod_{i=1}^N p(y_i | \beta)\right\}\\
&=-\log\left\{\prod_{i=1}^N \binom{m_i}{y_i} w_i(\beta)^{y_i} (1-w_i(\beta))^{m_i-y_i}\right\}\\
&=-\sum_{i=1}^N\log\left\{\binom{m_i}{y_i} w_i(\beta)^{y_i} (1-w_i(\beta))^{m_i-y_i}\right\}\\
&=-\sum_{i=1}^N\log\binom{m_i}{y_i} + y_i\log(w_i(\beta)) + (m_i-y_i)\log(1-w_i(\beta))\\
\nabla l(\beta)&=-\sum_{i=1}^N 0 + \frac{y_i}{w_i(\beta)} \nabla w_i(\beta) - \frac{m_i-y_i}{1-w_i(\beta)} \nabla w_i(\beta)\\
\nabla w_i(\beta)&=w_i^2(\beta) e^{-x_i^\top\beta} x_i\\
\nabla l(\beta)&=-\sum_{i=1}^N w_i^2(\beta) e^{-x_i^\top\beta} x_i\left(\frac{y_i}{w_i(\beta)}  - \frac{m_i-y_i}{1-w_i(\beta)}\right)\\
&=-\sum_{i=1}^N w_i^2(\beta) e^{-x_i^\top\beta} x_i\left(\frac{y_i-y_iw_i(\beta) - m_iw_i(\beta) + y_iw_i(\beta)}{w_i(\beta)(1-w_i(\beta))}\right)\\
&=-\sum_{i=1}^N w_i(\beta) e^{-x_i^\top\beta} x_i\left(\frac{y_i- m_iw_i(\beta)}{1-w_i(\beta)}\right)\\
&=-\sum_{i=1}^N w_i(\beta) \left(\frac{1}{w_i(\beta)} - 1\right)x_i\left(\frac{y_i- m_iw_i(\beta)}{1-w_i(\beta)}\right)\\
&=-\sum_{i=1}^N w_i(\beta) \frac{1-w_i(\beta)}{w_i(\beta)}x_i\left(\frac{y_i- m_iw_i(\beta)}{1-w_i(\beta)}\right)\\
&=-\sum_{i=1}^N \left[y_i- m_iw_i(\beta)\right]x_i\\
\end{align*}

\subsection{(B)}
See code on GitHub (\texttt{glm.R}).

\subsection{(C)}

\subsection{(D)}
Newton's direction is $-\left(\nabla^2l(\beta)\right)^{-1}\nabla l(\beta)$.

\end{document}