\documentclass{article}
\usepackage[top=.5in, bottom=.5in, left=.9in, right=.9in]{geometry}
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{mathtools}

\usepackage[authoryear,round]{natbib}




\newcommand{\obar}[1]{\ensuremath{\overline{ #1 }}}
\newcommand{\iid}{\ensuremath{\stackrel{\textrm{iid}}{\sim}}}

\newcommand{\op}[2]{{\ensuremath{\underset{ #2 }{\operatorname{ #1 }}~}}}
\newcommand{\norm}[1]{{ \ensuremath{ \left\lVert  #1 \right\rVert  }  }}


\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0,0.25,0}
\newcommand{\soln}{{\color{red}\textbf{Solution:~}\color{black}}}


\usepackage[formats]{listings}
\lstdefineformat{R}{~={\( \sim \)}}
\lstset{% general command to set parameter(s)
basicstyle=\small\ttfamily, % print whole listing small
keywordstyle=\bfseries\rmfamily,
keepspaces=true,
% underlined bold black keywords
commentstyle=\color{darkgreen}, % white comments
stringstyle=\ttfamily, % typewriter type for strings
showstringspaces=false,
numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt, %
frame=shadowbox,
rulesepcolor=\color{black},
,columns=fullflexible,format=R
} %
\renewcommand{\ttdefault}{cmtt}
% enumerate is numbered \begin{enumerate}[(I)] is cap roman in parens
% itemize is bulleted \begin{itemize}
% subfigures:
% \begin{subfigure}[b]{0.5\textwidth} \includegraphics{asdf.jpg} \caption{} \label{subfig:asdf} \end{subfigure}
\hypersetup{colorlinks=true, urlcolor=blue, linkcolor=blue, citecolor=blue}


\graphicspath{ {C:/Users/Evan/Desktop/} }
\title{\vspace{-6ex}HW 7\vspace{-2ex}}
\author{Evan Ott \\ UT EID: eao466\vspace{-2ex}}
%\date{DATE}
\setcounter{secnumdepth}{0}
\usepackage[parfill]{parskip}



\begin{document}
\maketitle

\section{Laplacian smoothing}

\subsection{(A)}
FIXME: do this


\subsection{(B)}
The problem is
\begin{align*}
\op{minimize}{x\in \mathbb{R}^n} \frac{1}{2}\norm{y-x}_2^2 + \frac{\lambda}{2}x^\top L x=\frac{1}{2}\left(x^\top x - 2y^\top x + y^\top y + \lambda x^\top L x\right)\\
\end{align*}
which we can find a solution for by taking the gradient w.r.t. $x$.
\begin{align*}
0&=\frac{1}{2}\left(2x - 2y + 0 + \lambda (L + L^\top) x\right)\\
&= \left(I + \frac{1}{2}\lambda (L + L^\top)\right)x - y\\
\left(I + \lambda L\right)\hat{x}  &= y
\end{align*}

\subsection{(C)}

\subsubsection{Gauss-Seidel}
Solving $Ax=b$ in this framework amounts to splitting $A=L_*+U$ where $L_*$ is the lower triangular matrix
(including the diagonal) and $U$ is the upper triangular matrix (excluding the diagonal).

The algorithm is then re-writing the problem iteratively as which can be solved with forward substitution.
$$L_* x^+ = b - Ux$$

See \cite{barrett1994templates} \S2.2.2 and Equation (2.6) (slightly different notation but same result).

\subsubsection{Jacobi}
Again solving $Ax=b$, we split into $A=D+R$ where $D$ is just the diagonals and $R$ is everything else (with 0 along the diagonal).

We then iterate
$$x^+ = D^{-1} (b-Rx)$$

What's nice here from an efficiency perspective is that $D$ is easily invertible (so long as there are no zeros in the diagonal), and potentially linearizable depending on the underlying code implementation.

See \cite{barrett1994templates} \S2.2.1 and Equation (2.4) (slightly different notation but same result).


\subsubsection{Conjugate Gradient}
$Ax=b$ is equivalent to minimizing $\phi(x)=\frac{1}{2}x^\top A x -b^\top x$ if $A$ is symmetric.

\begin{align*}
\nabla \phi(x) = Ax-b \equiv r(x)
\end{align*}

Then define $\{ p_0, p_1, \ldots, p_n\}$ as being conjugate w.r.t. $A$, such that
$p_i^\top A p_j = 0$ when $i\neq j$.

Now the algorithm proceeds as
\begin{align*}
x^{(k+1)} &= x^{(k)} + \alpha_k p_k\\
\alpha_k &= -\frac{r_k^\top p_k}{p_k^\top A p_k}
\end{align*}

So now we need to determine how to construct the $p_k$ vectors. You could use eigenvectors, but those
are in general pretty expensive to calculate. So now, let
\begin{align*}
p_0 &= -r_0\\
p_k &= -r_k + \beta_k p_{k-1}\\
\beta_k &= \frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}
\end{align*}



\subsection{Notes from class 10/31}
Next week: graph fused lasso.

Conjugate gradient is a lot more complicated, and is in fact solving a more general class of problems called Krylov subspace
problems. And it is \emph{fast} if the matrix falls into this
Laplacian class of matrices (certain properties). In that case, as opposed to a standard matrix inverse, $O(n^3)$ or
a sparse one, $O(n^2)$, it will actually be more like $O(n \ln n)$.

It turns out to be important to have a preconditioner. Solving $Ax=b$ is the same as solving $P^{-1}Ax=P^{-1}b$
where $P$ is a ``preconditioner.'' The closer $A$ is to $P$ (while $P$ is still easy to invert), the closer $P^{-1}A$ 
is to the identity, making the problem trivial. The current state of the art is the \emph{algebraic multigrid}
which is that $O(n \ln n)$ type solution.


\vspace{5ex} 

Notation for the rest of the notes:
\begin{align*}
C_\lambda\hat{x}&=y\\
\hat{x}&=C_\lambda^{-1}y
\end{align*}
so $\hat{x}$ is the smoothed/predicted $y$.

The question now is how to choose $\lambda$, potentially using $C_p$ or AIC/BIC, cross-validation, etc.

The leave-one-out lemma (from \cite{hastie2001elements})
allows us to calculate the LOOCV error. Assume $\hat{y}=Sy$
where $y,\hat{y}\in \mathbb{R}^n$ and $S$ is a smoothing matrix (so the linear case we care about).

We need the degrees of freedom of an estimator/model (basically number of free parameters).
\begin{align*}
y&=X\beta + \epsilon\\
\hat{y}&=X\hat{\beta}
\end{align*}
has $p$ degrees of freedom if $\beta\in\mathbb{R}^p$. We need to modify the definition to handle other cases.

Situations we need to address at a minimum:
\begin{enumerate}[1.]
\item Fit to $p$ variables (as an example, OLS)
\item Choose $p$ from $D>p$ candidate variables, then find the best fit for these $p$. This should have more degrees
of freedom if our definition is supposed to make sense at all.
\end{enumerate}

Suppose we have $\hat{y}$ such that $\hat{y}_i=g_i(y)$. Then
\begin{align*}
\textrm{df}(\hat{y}) &= \frac{1}{\sigma^2} \sum_{i=1}^n \textrm{cov}(\hat{y}_i, y_i)\\
\sigma^2&=\textrm{var}(y_i)
\end{align*}

Case: extreme overfitting: $\hat{y}_i=y_i$. Then
\begin{align*}
\textrm{df}(\hat{y}) = \frac{1}{\sigma^2}\sum_{i=1}^n \textrm{var}(y_i)=n
\end{align*}

Case: extreme underfitting: $\hat{y}_i=\obar{y}$ Then
\begin{align*}
\textrm{df}(\hat{y}) &= \frac{1}{\sigma^2}\sum_{i=1}^n \textrm{cov}(\obar{y}, y_i)\\
\textrm{cov}(\obar{y}, y_i)  &=  \textrm{cov}\left( \frac{1}{N}y_i + \frac{1}{N}\sum_{j\neq i}y_j, y_i \right)\\
&=\textrm{cov}\left(\frac{1}{N} y_i, y_i\right)=\frac{1}{N}\sigma^2\\
\textrm{df}(\hat{y})&=\frac{1}{\sigma^2}\sum_{i=1}^n \frac{1}{N}\sigma^2 = 1
\end{align*}

Case: linear smoothers: $\hat{y}=Sy$ then $\textrm{df}(\hat{y}) = \textrm{trace}(S)$.

\subsection{Picking up notes from last class}
Laplacian smoothing should have degrees of freedom between these two extremes.
For linear smoothers, $\hat{y}=Sy$ where $y,\hat{y}\in\mathbb{R}^n$. Then (where $S_i$ is the $i$-th row of $S$):
\begin{align*}
\hat{y}_i&=\langle S_i, y \rangle\\
\textrm{cov}(\hat{y}_i, y_i)&=\textrm{cov}\left( \sum_{j=1}^n S_{ij}y_j, y_i  \right)\\
&=\textrm{cov}\left( S_{ii}y_i + \sum_{j\neq i} S_{ij}y_j, y_i  \right)\\
&=\textrm{cov}\left( S_{ii}y_i, y_i  \right)\\
&=S_{ii}\textrm{cov}(y_i, y_i)=S_{ii} \sigma^2\\
\textrm{df}(\hat{y})&=\frac{1}{\sigma^2}\sum_{i=1}^n S_{ii}\sigma^2 =\textrm{trace}(S)
\end{align*}

Case: linear least squares: $\hat{y}=X\beta+\epsilon$, where $\hat{\beta}=(X^\top X)^{-1}X^\top y$.

In linear regression, we would expect this to have $p$ degrees of freedom. This is a linear smoother itself, as
$$\hat{y}=X\hat{\beta}=X(X^\top X)^{-1}X^\top y$$
So $\textrm{df}(\hat{y})=\textrm{trace}(X(X^\top X)^{-1}X^\top)$. This is a
projection matrix, so the trace is the rank, and the rank is the dimension of the
underlying subspace, and this is $p$. Or, we can use the properties of trace.

\begin{align*}
\textrm{trace}(ABC)&=\textrm{trace}(CAB)\\
\textrm{trace}(X(X^\top X)^{-1}X^\top)&=\textrm{trace}(X^\top X(X^\top X)^{-1})\\
&=\textrm{trace}(I_p)=p
\end{align*}

It would be nice to be able to calculate the degrees of freedom of the lasso, but
this result is hard to use in practice. Instead, we can use Stein's lemma.
This shows that for a pretty general class of estimators, we can instead
use an alternate formula to compute the degrees of freedom and it is equivalent
to the formula above. See \cite{zou2007degrees} to see where this lemma
is used to find the degrees of freedom of lasso.

The more philosophical question is how the degrees of freedom of linear
regression and lasso are both $p$ (where $p$ for the lasso are the non-zero coefficients). More-or-less, this is because lasso also shrinks the variables
it estimates. So the extra variables in lasso that can be selected cause an 
increase in degrees of freedom that is exactly
canceled because of shrinkage.

Why do we even need the degrees of freedom? For example, it's for model
selection using $C_p$ or AIC/BIC, etc.

\subsubsection{Graph-fused lasso}
The degrees of freedom in this problem is the number of distinct constant
regions in the fitted $\hat{x}$.

In terms of actually solving the problem, we have
\begin{align*}
\op{minimize}{x\in \mathbb{R}^n}&  \frac{1}{2}\norm{y-x}_2^2 + \lambda \norm{r}_1\\
\textrm{subject to}~& Dx-r=0
\end{align*}
Doing the normal ADMM will be fine, but you'll need to refactor a matrix
every time. In the advanced version, we start from this same constrained
optimization problem but move the constraint back into the objective.

\begin{align*}
\op{minimize}{x\in \mathbb{R}^n}&  \frac{1}{2}\norm{y-x}_2^2 + \lambda \norm{r}_1+I_C(x,r)\\
I_C(x,r)&=\left\{
\begin{matrix}
\infty & Dx\neq r\\
0 & \textrm{o.w.}
\end{matrix}
\right.
\end{align*}
So now, we'll extend this to have have a slack variable for $x$ and one for $r$.
\begin{align*}
\op{minimize}{x\in \mathbb{R}^n}&  \frac{1}{2}\norm{y-x}_2^2 + \lambda \norm{r}_1+I_C(z,s)\\
\textrm{subject to}~& x-z=0 \wedge r-s=0
\end{align*}

However, it turns out that the $x$ update is the most computationally expensive
part. In the original ADMM, the $x$ update is generically $A^{(t)}x=b^{(t)}$
but in this final form, it is $Ax=b^{(b)}$ so we can re-use the same
matrix decomposition for the $x$ update. At this point, all the updates
will be a lot more cheap, including the $x$ update, so everything will be considerably faster.



\section{Graph fused lasso}
Switching to an ADMM framework, we have
\begin{align*}
\op{minimize}{~}&  \frac{1}{2}\norm{y-x}_2^2 + \lambda \norm{r}_1\\
\textrm{subject to}~& Dx-r=0
\end{align*}

However, in this case, we can re-write the ADMM using additional slack
variables as noted in the notes in the previous section. For additional
information, see \S5 in \cite{tansey2014false}. So now, use the following:
\begin{align*}
\op{minimize}{~}&  \frac{1}{2}\norm{y-x}_2^2 + \lambda \norm{r}_1+I_C(z,s)\\
\textrm{subject to}~& x-z=0 \wedge r-s=0
\end{align*}
to which we introduce scaled slack variables $u$ for $x-z=0$ and $t$ for $r-s=0$:

\begin{align*}
\op{minimize}{~}&  \frac{1}{2}\norm{y-x}_2^2 + \lambda \norm{r}_1+I_C(z,s) + \frac{a}{2}\norm{x-z+u}_2^2+\frac{a}{2}\norm{r-s+t}_2^2
\end{align*}

\subsection{$x$ update}
The augmented Lagrangian is separable in $x_i$ so for each $x_i$ we have:
\begin{align*}
\op{minimize}{x_i\in\mathbb{R}}&  \frac{1}{2}(y_i-x_i)^2 + \frac{a}{2}(x_i-z_i+u_i)^2\\
0&=x_i-y_i + a(x_i-z_i+u_i)\\
\hat{x}_i&=\frac{1}{1+a}\left(y_i+az_i-au_i\right)
\end{align*}
Or as a vector,
$$\hat{x}=\frac{1}{1+a}\left(y+az-au\right)$$

\subsection{$r$ update}
The augmented Lagrangian is also separable in $r_i$ so we have:
\begin{align*}
\op{minimize}{r_i\in\mathbb{R}}&   \lambda |r_i| +\frac{a}{2}\left(r_i-s_i+t_i\right)^2
\end{align*}
As shown many times before, the solution is simply the soft-thresholding function:
$$\hat{r}_i=S_{\lambda/a}\left(s_i-t_i\right)$$

\subsection{$z, s$ updates}
Because of the indicator function, $z$ and $s$ must be considered jointly (indeed, as vectors, not as elements because the constraint can't in general
be checked without all values).

\begin{align*}
\op{minimize}{z\in\mathbb{R}^n,~ s\in\mathbb{R}^m}& I_C(z,s) + \frac{a}{2}\norm{x-z+u}_2^2+\frac{a}{2}\norm{r-s+t}_2^2
\end{align*}
We again play tricks and move the indicator part of the objective back to a constraint to remove $s$ from the objective.
\begin{align*}
\op{minimize}{z\in\mathbb{R}^n}& \frac{a}{2}\norm{x-z+u}_2^2+\frac{a}{2}\norm{r-Dz+t}_2^2\\
\textrm{subject to}~& Dz-s=0
\end{align*}
First, simplify notation such that $w=x+u$ and $v=r+t$.
The objective can now be re-written as:
$$\frac{a}{2}\left[z^\top z - 2w^\top z + w^\top w + z^\top D^\top D z
-2v^\top D z + v^\top v\right]$$
which is simply a quadratic form in $z$, so
\begin{align*}
0&=\frac{a}{2}\left[ 2z -2w + 2D^\top D z - 2D^\top v \right]\\
0&=(I+D^\top D)z - w - D^\top v\\
\hat{z}&=(I+D^\top D)^{-1}(w+D^\top v)=(I+D^\top D)^{-1}\left(x+u+D^\top[r+t]\right)
\end{align*}
Then set $\hat{s}=D\hat{z}$.

\subsection{$u, t$ updates}
Finally, the scaled slack variables are updated as:
\begin{align*}
\hat{u}&=x-z+u\\
\hat{t}&=r-s+t
\end{align*}

\subsection{All updates together}
Placing everything together, we have:
\begin{align*}
x^{(k+1)}&=\frac{1}{1+a}\left(y+az^{(k)}-au^{(k)}\right)\\
r_i^{(k+1)}&=S_{\lambda/a}\left(s_i^{(k)}-t_i^{(k)}\right)~~~i\in\{1,\ldots, m\}\\
z^{(k+1)}&=(I+D^\top D)^{-1}\left(x^{(k+1)}+u^{(k)}+D^\top[r^{(k+1)}+t^{(k)}]\right)\\
s^{(k+1)}&=Dz^{(k+1)}\\
u^{(k+1)}&=x^{(k+1)}-z^{(k+1)}+u^{(k)}\\
t^{(k+1)}&=r^{(k+1)}-s^{(k+1)}+t^{(k)}
\end{align*}

\subsection{Convergence}
I used the convergence criteria spelled out in \cite{boyd2011distributed} \S3.3.1 with $\epsilon^{\textrm{abs}}=10^{-8}$ and $\epsilon^{\textrm{rel}}=10^{-5}$

\subsection{Notes from class 11/7}
Problem: James originally said that the original ADMM version would require re-factorizing the matrix for the $x$
update, but that ends up only being the case if you have a weighted form $\frac{1}{2}\norm{W^{1/2}(y-x)}_2^2$ where $W$ may depend on the iteration number (such as being a function of $x$). In this case, that matrix is just the
just the identity, so it's fine.

To do cross validation, basically, just throw out things in the L2 norm term, but keep them in the L1 term. Preserves smoothing.

Can re-write the objective as
\begin{align*}
\op{minimize}{x\in\mathbb{R}^n} &\frac{1}{2}\norm{y-x}_2^2+\lambda \sum_{(i,j)\in E} |x_i-x_j|\\
\op{minimize}{x\in\mathbb{R}^n} &\frac{1}{2}\norm{y-x}_2^2+\lambda \sum_{(i,j)\in R} |x_i-x_j|+\lambda \sum_{(i,j)\in C} |x_i-x_j|\\
R\cup C &= E, ~~\textrm{R is edges along rows, C is edges along cols}
\end{align*}
We then split along the edges in row 1, row 2, up to row D. Pretend we didn't have the column terms just for a moment.
We could solve each row in parallel then. There's an algorithm for doing this chain graph-fused lasso that's similar
to hidden Markov model Kalman filtering algorithms and it runs in linear time.

So, we just use a new variable, $z$ for the column terms and add the constraint $z=x$. So, we just do ADMM.
This is proximal stacking as in \cite{barbero2014modular}. James has code on the class repo to show how to use it.





\subsection{Notes from class 11/9}
``Data-analysis applications of the fused lasso and related spatial smoothers'' presentation.

Recurring problem: $\op{minimize}{\beta} l(\beta)+\lambda \norm{D\beta}_1$.

Toy example of chain graph fused lasso shows that coefficients get pulled toward the mean, may have a lot more levels than the underlying truth. In a way, it's shrinking the estimate of the \emph{jumps} between segments.

\subsubsection{Trend filtering}

$$\op{minimize}{\beta \in \mathbb{R}^N} \frac12 \norm{y-\beta}_2^2+\lambda \norm{D^{(k+1)}\beta}_1$$

So $D^{(k+1)}$ is a discrete analog of the order-$k$ derivative. Each one is like the ``differences of the differences (of the differences $\ldots$).'' You plug in the order of the derivative that you care about, so you're imposing a form on $\beta$. $k=0$ is piecewise constants. $k=1$ is piecewise linear. And so forth. There's a little subtlety because you lose a row each time you increase $k$.

This is different than splines. Splines are piecewise polynomial functions such that you get derivatives
at all of the breakpoints up to the order of the polynomial minus 1. Why is this different than regression splines?
In regression splines, you pick the breakpoints then fit by least squares. In the graph-fused lasso, you can change
the regression at \emph{any} data point, rather than selecting the breakpoints ahead of time. You also select the breakpoints jointly. You also need basis splines such as $b$-splines. In this problem, you get a falling-factorial basis which is related but not the same basis.

There's a fast way of solving the trend filtering, see \cite{ramdas2015fast}. See also \texttt{glmgen} on GitHub
for implementation of this.

Can generalize to a general graph instead of a chain. See \cite{wang2015trend} for how to generate the $D$ matrix
in the general case.

\subsubsection{Spatially aware anomaly detection}
This I think is partially Alex Reinhart's work with the radiation detector. In the presence of an anomaly, not only
is the background rate going to change, but the energy spectrum is going to change. One simple thing
to do to test is comparing the background (known) and the new empirical distribution with a Kolmogorov-Smirnoff 
difference of distribution test. 

Recursive dyadic partitions. Instead of taking just bins, split the space in half and count each side. Recursively split.
Now, we try to smooth the values in each split across space. This is embarassingly parallel to solve.




\bibliographystyle{plainnat}
\bibliography{/Users/Evan/GitProjects/tex-docs/references}


\end{document}